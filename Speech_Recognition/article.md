## Speech Recognition

![image](https://github.com/Tudor-Gavriliuc/articles/assets/120340209/f7cb8f77-d19b-4b3e-b744-f014bf562a49)

Speech recognition is not just about the algorithms we apply to get a list of words from an audio file. It is a complex topic that includes several key ideas and in the following article I will cover the most important one.


## Key words:

Amplitude, Frequency, Vocal Cords, Phoneme.

- Amplitude is the maximum extent of a vibration or oscillation, measured from the position of equilibrium.
- Frequency represnts the number of times something happens within a particular period, or the fact of something happening often or a large number or times.
- The vocal cords (also called vocal folds) are two bands of smooth muscle tissue found in the larynx (voice box). The vocal cords vibrate and air passes through the cords from the lungs to produce the sound of your voice.
- A phoneme is any of the perceptually distinct units of sound in a specified language that distinguish one word from another, for example p, b, d, and t in the English words pad, pat, bad, and bat.

## *How do sounds appear?*
(The material played in the following video can be found below it)

https://github.com/Tudor-Gavriliuc/articles/assets/120340209/ace079b1-5faf-49a5-ace8-ae34c738d3e5

// How does the human body produce voice and speech? Every day many of us rely on voice and speech to communicate. Let’s explore the processes of breathing, voicing, and speaking. What is voice? [BABY COOING] Your voice is as unique as your fingerprint. [MAN SINGING] Voice, or vocalization, is the sound produced by humans and other vertebrates using the lungs and the vocal folds in the larynx. [WOMEN LAUGHING] The larynx, sometimes called the voice box, is the primary organ of voice production. [TODDLER LAUGHING] Voice and speech are not the same, an infant crying or an adult laughing are examples of vocalization or voice production. [MAN LAUGHING] What is speech? People typically express thoughts, feelings, and ideas orally to one another through a series of complex movements that alter the basic tone created by voice into specific, identifiable sounds called speech. Before we get into more detail about how voice and speech are produced, let’s explore how we breathe. The first step is taking air into the lungs. The large dome shaped muscle called the diaphragm, which is located directly below the lungs, contracts and expands vertically lifting the rib cage, and allowing the lungs to fill with air. After the lungs bring oxygen into the body, they remove carbon dioxide through breathing out. When breathing out, the diaphragm and ribcage return to their resting positions, forcing the air out of the lungs. How do we produce voice? There are several parts of the body that play a role in voice production. Vocal folds are two bands of tissue that are positioned opposite each other in the larynx. The larynx is located between the base of the tongue and the top of the passageway to the lungs, known as the trachea. When you are not speaking, the vocal folds are open so that you can breathe. Voice is generated by airflow from the lungs. When it’s time to speak, the pressure below the larynx increases until it blows the vocal folds apart. [MAN: HELLO, HELLO] The vocal folds are first forced apart at the bottom followed by the top, and air is forced through the very narrow opening. The opening between the vocal folds is called the glottis, which opens during breathing and narrows during voice production. The pressure forces the air through the vocal folds which creates a suction as we exhale. This suction pulls the vocal folds together at the bottom, followed by the top. When the air from the lungs blows past the vocal folds at a high speed, the vocal folds vibrate. In order to produce typical, loud speech, the vocal folds and larynx must vibrate normally. How do we produce speech? Speech is produced by precisely coordinated muscle actions in the head, throat, chest, mouth, nasal cavity, and abdomen. Speech development is a gradual process that requires years of practice. During this process, a child learns how to regulate these muscles to produce understandable speech. [TODDLER MAKING SPEECH SOUNDS] Different parts of our nose and mouth are carefully positioned to create speech sounds. We position the velum, tongue, jaw, and lips in reference to permanent parts of our vocal tract, such as the roof of our mouth. [MAN: HELLO, HELLO] As you can see, the process of producing voice and speech is complex and plays an important role in our daily life. For more information on voice and speech, visit the NIDCD website at www.nidcd.nih.gov //
 
The primary source of speech sounds in most languages is the human vocal tract. It consists of the lungs, larynx (voice box), pharynx, oral cavity (mouth), and nasal cavity. Different speech sounds are produced by manipulating airflow, vocal cord vibrations, and the shape of the vocal tract.

![image](https://github.com/Tudor-Gavriliuc/articles/assets/120340209/e90c018d-8cb5-428a-9708-1018c35703fa)


## *From analog to digital representation*

The first step in analog-to-digital conversion is sampling, where the continuous analog speech signal is converted into discrete samples taken at regular intervals. The sampling rate, such as 8 kHz or 16 kHz, determines how frequently these samples are taken, representing the original analog signal more accurately in the digital domain.

![image](https://github.com/Tudor-Gavriliuc/articles/assets/120340209/0e2d6582-2d96-4fa2-bb53-c9ad0f36347c)

After sampling the analog signal, the next step is quantization. This involves converting each sampled value from the continuous amplitude range into a discrete digital value using binary numbers. The bit depth determines the number of bits used to represent each sample, with higher bit depths providing more precision but requiring more storage and resources. Common bit depths in speech recognition are 8, 16, and 24 bits. The digital values are then encoded into formats like PCM, used widely in speech recognition. The result is a digital signal composed of discrete samples, which can be stored or processed by speech recognition algorithms.
https://youtu.be/JARmZM4FH8Q

## *Fast Fourier Transform (FFT)*
(The material played in the following video can be found below it)

Speech signals are time-varying, and the information about the sounds and phonemes is often encoded in the frequency domain. FFT allows us to transform the speech signal from the time domain to the frequency domain, revealing the specific frequencies and their magnitudes present in the signal.

https://github.com/Tudor-Gavriliuc/articles/assets/120340209/2ffbe853-b0d8-4d0d-8ccb-a44526fb4f4f

// Can you tell which song this is what about now [Music] still not okay here's one final round before I reveal the song [Music] this song guessing game was made using the op math concept of the Fourier transform the to see the beauty of the Fourier transform in action as well as teach you a bit about sound songs and speakers I'll walk you through a road map of exactly how this game was made the first step is to download the WAV file for a song a WAV file is a 1D array of values between negative one and one these values correspond to the displacement of the speaker from some rest position 0 as a function of time remember you hear sound because pressure waves caused by a speaker are hitting your ear and here's a cool way to visualize those pressure waves using a speaker and a candle after storing a wave file in an array it was time to perform a Fourier transform a Fourier transform allows you to decompose any wave into a series of infinite sine waves in this simple Example The Wave shown in purple was created using just three sine waves with frequencies 0.2 2 and 3 Hertz all of the other frequencies have amplitudes of zero so they aren't shown it's worth noting that the amplitude of each of these waves is different and you can clearly see that the Blue Wave which has the largest amplitude out of the three has the largest influence on the general shape of the wave for a more in-depth view on how the Fourier transform actually works I'll link videos from veritasium and three blue one Brown in the description their videos on this subject are fantastic and helped Inspire this project anyway after doing the Fourier transform you know the amplitude and frequency of each of the sine waves this data can best be summarized by plotting the amplitude versus the frequency of all these individual sine waves this is called the frequent Spectrum as you could have predicted because I told you like 15 seconds ago there are only three frequencies with non-zero amplitudes I didn't want to show these peaks with infinite slopes so that's why the peaks look like triangles by the way the values in this 1D array of data are actually complex numbers because you need to know the amplitude and the phase shift of each individual sine wave but I'll just keep referring to it as the amplitude for Simplicity this was for an example continuous wave but the wave of any real audio file is not continuous but rather a bunch of discrete points for the discrete version of our original example wave you could perform a Discrete Fourier transform instead of an infinite number of sine waves the number of sine waves is dictated by the number of points in the original sample that means rather than being a continuous function the frequency spectrum will be a bar chart where each bin is one frequency as expected they're still the same three frequencies with non-zero amplitudes for a five second audio clip sampled at 44.1 kilohertz that would be two hundred and twenty thousand five hundred which would be decomposed into two hundred and twenty thousand five hundred different frequencies each with a different amplitude now that we have this data we could just combine all these individual ways back together using the Uno reverse Fourier transform also known as the inverse Fourier transform but that would just create the original song but the song guessing game we want to filter the data such that we only use a certain percentage of these frequencies since the frequency is whose amplitudes are the largest have the largest influence on the overall shape of the original wave we sort the data by the largest amplitude and set the amplitudes of the frequencies we don't want to use to zero in this manner we can start with a single frequency and then add batches of the most impactful frequencies each round to progressively make the wave look more and more like the original this makes the song Sound more and more like the real song Until It's final View foreign [Music] thanks for watching if you want to try and guess more songs go check out another Channel I just made where I'll be posting more songs and any additional math music and art content that comes up in the future good luck and I'll see you next time. //

## *Feature Extraction*

The crucial speech recognition process of feature extraction includes breaking down the digital voice signal into a collection of significant and representative features. These characteristics act as inputs to voice recognition algorithms, enabling them to efficiently recognize and distinguish between various spoken words or phonemes. The following steps are commonly involved in the feature extraction process:

Preprocessing: The digital voice signal may go through some preprocessing stages to improve its quality and reduce noise before feature extraction. Filtering, normalizing, and silence elimination are common preprocessing methods.

Frame blocking is the process of breaking up a continuous digital speech output into smaller, overlapping frames. A brief segment of the signal, often lasting 20 to 30 milliseconds, makes up each frame. When frames overlap, the flow of information is maintained.

[Speech Recognition — Feature Extraction MFCC & PLP](https://jonathan-hui.medium.com/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9)

![image](https://github.com/Tudor-Gavriliuc/articles/assets/120340209/1b80cdd5-e7cd-42f4-bab3-5fd1e8101208)

## *Acoustic Modeling*
**Input**
A speech signal.
**Output**
A recognized sequence of phones (the basic unit of speech).


[A Basic Introduction to Speech Recognition (Hidden Markov Model & Neural Networks)](https://www.youtube.com/watch?v=U0XtE4_QLXI) (Below you can find the material played in this video)

// Hi, I'm Hannes. Hey, I'm Rodolphe. And last week together we made a workshop about speech recognition at our job and one of the parts of that workshop was about the technology behind it and to be honest it's not our field of expertise at all so we had a hard time reading a lot of stuff, a lot of difficult stuff, and we never found a video that explains it easily and now that we put a lot of effort in it, maybe it's time that we put this video online so here you have it: A Speech Recognition for Dummies. Rodolphe: Oh yeah, and by the way last week we had the help of two experts called e-Rodi and e-Hanni and those two guys are also gonna come back to help us explain that to you today. Enjoy! All right, so it all starts with us humans making sound, making noise in a normal environment. Actually, technically we call that an analog environment and the thing is that the computer cannot work with analog data. It needs digital data to be to be able to work with so that's why the first piece of software that we need is what we call an analog to digital converter. Actually, we can name that also a microphone. Hey, e-Rodi, can you help the people here understand how a microphone works? E-Rodi: Okay sure I'll help. I'll pretend to be a microphone converting a sentence from analog to digital. Which phrase do you want me to use? Rodolphe: How you doing? E-Rodi: All right, let's go! In order for you humans to see the conversion, we computers use a visualization called spectrogram. To create the spectrogram three main steps are needed. First, I capture the sound wave and placed it in a graph showing its amplitude over time. As you can see the amplitude units are decibels and we can even guess here the three words that you just said. Second, I chop this wave into blocks of approximately a second. I'm not really that good of a microphone to be honest but I've colleagues that can make this blocks much thinner. As you can see the height of the blocks are determining its state. To each state we can allocate a number and a number being something digital we have successfully converted this sound from analog to digital. Two steps down, one more to go! Even if the data is digitized, we are still missing something. In the speech recognition process we actually need three elements of sound: its frequency, its intensity and the time it took to make it. Therefore we will be using a super complex formula called Fast Fourier Transform to convert the graph you're currently seeing into what we call a spectrogram. To ease your understanding I show you here both a handwritten version and a computer made version of the spectrogram. As you can see the spectrogram shows the frequency on the vertical axis and the time on the horizontal axis. And the colors are actually the energy that you use to make the sound. The brighter the color, the more energy was used. The last interesting fact about the spectrogram is the time scale. As you can see it's way more precise: each vertical line is between 20 to 40 milliseconds long and is called an acoustic frame. Okay, now it's time to really be honest. I'm a good analog to digital converter but actually my job stops there. Although I managed to have a digital version of the sounds you made, I have no idea whatsoever what they're supposed to mean. If they even mean something. So I suggests that my colleague e-Hanni tells you all about how the computers can understand the meaning of sound. But before that our real-life versions have to do a little work. Let's split the work like this: you guys explain the concept of phonemes and we take over the rest of the heavy explanation. Rodolphe: You got yourself a deal. All right, it's time for a little introduction to linguistics. What is a phoneme? A phoneme is as small as 20 to 40 milliseconds, so it's super super short and it's a unit of sound that distinguishes one word from another in a particular language. To put it differently, it's the tiniest part of the word that you can change and that also makes the meaning of that word change. For instance, the word thumb and the word dumb are two different words that you can distinguish by the substitution of one phoneme 'th' by another phoneme 'd'. Those phonemes can be spoken differently by different people but it's always the same phoneme that is meant. Those variations are what we call allophones. And the reasons of those variations are the accent of the person its age, the gender, the position of the phoneme within the word or even the emotional status of the speaker. Those phonemes are important because they are the very basic building blocks that the speech recognition software can use to actually put them in the right order to first form a word, then afterwards a sentence and etc. So the speech recognition software does that by using two techniques: the Hidden Markov Model and the Neural Networks. All right, so I'll explain those two. And let's maybe first start with the Hidden Markov Model. So as Rodolphe just said, it's actually the purpose to reconstruct the phrase that has just been said so by putting the right phonemes after each other and the Hidden Markov Model does that by using statistical probabilities. So we will check how probable it is that one phoneme follows after the other and so on. To be precise: the Hidden Markov Model does that using three different layers and maybe, e-Hanni, you can help us by visualizing this? E-Hanni: Okay sure, I'll help. I'll pretend to be a speech recognition system using a Hidden Markov Model. Which phrase do you like me to use? Hannes: Dolphins swim fast. E-Hanni: All right, let's go. So first of all the model has to check on an acoustic level the probability that the phoneme it has heard really is that phoneme. So that means, as Rodolphe just said, we say phonemes in a very different way according to emotion position in the phrase, and so on. And so the system first needs to check whether that variation it has heard in a phoneme really is that phoneme. E-Hanni: Okay, so the first utterance I recorded in the phrase of Hannes was d. Statistically seen, Hannes could have said 't', 'th', or 'd'. But most likely, most probably, it was a 'd'. So let's take that one. So, once the software has reached a decent probability of what the most likely said phoneme is, then it is time to go to the second layer. And in the second layer the Hidden Markov Model will actually check whether phonemes next to each other, if it's probable that they are standing next to each other, yes or no. So maybe an example in English: if you have the sound 'st', then it's most likely that a vowel will follow for example an 'a', such as in stable and it's less likely, or maybe not even possible in English, to have the sound 'n' after it because 'stn': I don't think it exists and if it does then it's not probable. E-Hanni: Ok, so after the 'd' I heard an 'o'. Statistically seen it is actually quite probable that an 'o' followed after the 'd', so let's keep it that way. After that I've also heard an 'l' and again it's quite probable that an 'l' follows after an 'o'. So, I think I've put together the first phonemes to make a word. The word 'doll'. Hannes: let's see about that, e-Hanni! Because in the third layer now the software will check on word level, so it will check whether words standing next to each other if that's probable and if it makes sense. So for example, it will also check if there are too many verbs or too few verbs in the phrase, it needs adverbs, if there are enough subjects in it, and so on... E-Hanni: Well, I think I already have to go back to the second layer again because while you were talking I've put together the second word and it's Fins. But 'doll Fins', it doesn't really make sense. So let me go back to the second layer and reassess... Ah, you probably said dolphins. Alright! Now the next phonemes I've put together made the word 'swim' and the word 'passed'. But now my phrase doesn't really make sense, because I have two verbs. So let me maybe check 'passed' again. I need to find an adverb that sounds like 'passed' so that my phrase is grammatically correct. So let me go back to the previous layers again and... I already see it. It seems like the 'p' in the first layer could also be 'f' and then it makes 'fast'. Dolphin swim fast! Hannes: That's right, e-Hanni. Now people who sometimes dictate to their phone, they may already have seen this happening. So the more input you give to your phone, then it may be that sometimes words in the beginning of your phrase start changing because the system has become wiser, it knows what you're trying to say, or not trying to say, and that's why it changes some words. E-Hanni: So in short about the Hidden Markov Model. It has a great fit with a sequential nature of speech. However it's not that flexible Also, all the varieties of the phonemes, it cannot really grasp it, it's too much. Hannes: All right, next to the Hidden Markov Model, we also have the Neural Network. So let's maybe talk a bit about that one. And a good thing about the Neural Network is that this one is flexible. So as the name says itself, the Neural Network is actually, the working of it, is based on how our brain works. So it's with a lot of nodes that are all connected with each other. And maybe let's visualize again, so e-Hanni, can you help us? E-Hanni: Yes! So a Neural Network is built up by an input layer, a hidden layer, and an output layer. The middle layer can be composed of many different layers. Now, as you can see, the connections all have different weights, ... so that only the information that passes a certain threshold will be sent through to the next one. Next to that it also means that if a node has to choose between two inputs so here C has to choose between the input of A or B, then it will choose the input of the node with which it has the strongest connection. So in this case it will take the information from A. Sometimes, in some systems, it can also take both inputs and that makes a ratio of it. So here you can see that it takes most of the input of A, but also a little bit of the input of B. Hannes: The interesting thing about Neural Networks is that it's flexible, so it can change over time. This means that in the beginning we have to train the Neural Network which also means that in the beginning all the different connections have the same weight. E-Hanni: Yes, indeed! So here you can see an empty neural network so that means that everything has the same weight. So we will give a certain input to the neural network and we will say what's the desired output is. Then we will let the neural network do its thing and it will come up with a certain put which is of course not the same as desired outputs. Because it's still young, it still needs to be trained. The difference between that we call the error. We also tell it to the Neural Network, that there is an error. From that point the Neural Network can start adapting itself, ... so that it can make the error smaller. Now, for the Neural Network to improve, to keep improving, it needs a lot a lot of inputs to make the error go away. Hannes: And that's a downside. Another downside is that it has a bad fit with a sequential nature of speech but on the plus side, as already said, it's flexible and it can also grasps the varieties of the phonemes and with that I mean that it can see a difference between unique voices, emotions, phonemes in the beginning or at the end of the phrase, and so on. So that's really good. Now I think it's for e-Hanni to do the conclusion. Right, e-Hanni? E-Hanni: These plus and downsides are very compatible with the plusses and downs of the Hidden Markov Model. That is why the Hidden Markov Model and the neural networks are often combine nowadays. So that's why we talk about a... hybrid! So that was it. Actually, we tried to put all the difficult parts in a coherent story and now we hope you enjoyed it and if you're interested about it, please go look it up on the Internet. Ciao! Bye bye! //

In voice recognition systems, acoustic models are a particular kind of machine-learning model. The model is trained to identify the acoustic features of human speech and provide an appropriate text transcription. Huge datasets of spoken words, phrases, and sentences are often used to train acoustic models. To “map” the sounds of human speech, these datasets are used.

![image](https://github.com/Tudor-Gavriliuc/articles/assets/120340209/c7036b6b-e697-4411-b291-17c2e3294c69)

## *Hidden Markov Model ( HMM)*

A Hidden Markov Model (HMM) is a mathematical model that deals with sequences of observations, where each observation is tied to a hidden state. The model uses probabilities to transition between states and emit observations. It’s commonly used in tasks like speech recognition or DNA sequence analysis. The HMM’s goal is to find the best sequence of hidden states that explains the observed data. It learns from training data and is used for decoding sequences in various applications.

![image](https://github.com/Tudor-Gavriliuc/articles/assets/120340209/01e14069-c487-4cd3-a31b-e6af7c60c27c)

## *Hidden Markov Model ( HMM)*

```
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from hmmlearn import hmm
  
# Define the state space
states = ["Word0", "Word1", "Word2", "Word3"]
n_states = len(states)
  
# Define the observation space
observations = ["Loud", "Soft"]
n_observations = len(observations)
  
# Define the initial state distribution
start_probability = np.array([0.8, 0.1, 0.1, 0.0])
  
+transition_probability = np.array([[0.7, 0.2, 0.1, 0.0],
                                    [0.0, 0.6, 0.4, 0.0],
                                    [0.0, 0.0, 0.6, 0.4],
                                    [0.0, 0.0, 0.0, 1.0]])
  
# Define the observation likelihoods
emission_probability = np.array([[0.7, 0.3],
                                  [0.4, 0.6],
                                  [0.6, 0.4],
                                  [0.3, 0.7]])
  
# Fit the model
model = hmm.CategoricalHMM(n_components=n_states)
model.startprob_ = start_probability
model.transmat_ = transition_probability
model.emissionprob_ = emission_probability
  
# Define the sequence of observations
observations_sequence = np.array([0, 0, 0, 0, 1, 1, 0, 1]).reshape(-1, 1)
  
# Predict the most likely hidden states
hidden_states = model.predict(observations_sequence)
print("Most likely hidden states:", hidden_states)
  
# Plot the results
sns.set_style("darkgrid")
plt.plot(hidden_states, '-o', label="Hidden State")
plt.legend()
plt.show()

```
![image](https://github.com/Tudor-Gavriliuc/articles/assets/120340209/b5fedd8b-593d-46b3-8307-005a8719219a)
- In our case (“Speech recognition”) observation is the audio file that we take as input, and the states are the [phonemes,words] we need to find out.

## *Language Modeling*

- A language model is a probabilistic model that assigns probabilities to any sequence of words.
- A good language model has to assign high probabilities to well performed sentences.
- 
  **Statistical Approach (N-grams):**

- Traditional language models, like N-gram models, estimate the likelihood of a word based on its previous N-1 words.
- The model calculates probabilities of word sequences based on the frequency of co-occurrences in the training data.
- For example, a bigram model predicts the next word based on the previous word, while a trigram model considers the previous two words.

  ![image](https://github.com/Tudor-Gavriliuc/articles/assets/120340209/597f009c-eaed-4e4c-9b11-1eb52fdcbd4b)

## *Decoding*
- Combine the outputs of the acoustic model and the language model to decode the most likely transcription for the given audio.
- Apply algorithms like Viterbi or beam search to find the best sequence of words that match the audio features.

  *The code below shows how the Viterbi algorithm works in the context of speech recognition.*

```
# Define possible observations and states
obs = ("sound1", "sound2", "sound3")
states = ("Word1", "Word2")

# Define initial state probabilities
start_p = {"Word1": 0.6, "Word2": 0.4}

# Define transition probabilities between states
trans_p = {
    "Word1": {"Word1": 0.7, "Word2": 0.3},
    "Word2": {"Word1": 0.4, "Word2": 0.6},
}

# Define emission probabilities of observations from states
emit_p = {
    "Word1": {"sound1": 0.5, "sound2": 0.4, "sound3": 0.1},
    "Word2": {"sound1": 0.1, "sound2": 0.3, "sound3": 0.6},
}

# Define the Viterbi algorithm function
def viterbi(obs, states, start_p, trans_p, emit_p):
    V = [{}]  # Initialize Viterbi matrix

    # Initialization for t = 0
    for st in states:
        V[0][st] = {"prob": start_p[st] * emit_p[st][obs[0]], "prev": None}

    # Run Viterbi for t > 0
    for t in range(1, len(obs)):
        V.append({})
        for st in states:
            max_tr_prob = V[t - 1][states[0]]["prob"] * trans_p[states[0]][st] * emit_p[st][obs[t]]
            prev_st_selected = states[0]
            for prev_st in states[1:]:
                tr_prob = V[t - 1][prev_st]["prob"] * trans_p[prev_st][st] * emit_p[st][obs[t]]
                if tr_prob > max_tr_prob:
                    max_tr_prob = tr_prob
                    prev_st_selected = prev_st

            max_prob = max_tr_prob
            V[t][st] = {"prob": max_prob, "prev": prev_st_selected}

    # Print the Viterbi matrix
    for line in dptable(V):
        print(line)

    # Backtrack to find the most likely sequence of states
    opt = []
    max_prob = 0.0
    best_st = None
    for st, data in V[-1].items():
        if data["prob"] > max_prob:
            max_prob = data["prob"]
            best_st = st
    opt.append(best_st)
    previous = best_st

    # Follow the backtrack till the first observation
    for t in range(len(V) - 2, -1, -1):
        opt.insert(0, V[t + 1][previous]["prev"])
        previous = V[t + 1][previous]["prev"]

    # Print the result
    print("The steps of states are " + " ".join(opt) + " with the highest probability of %s" % max_prob)

# Define a function to print the Viterbi matrix as a table
def dptable(V):
    yield " " * 5 + "     ".join(("%3d" % i) for i in range(len(V)))
    for state in V[0]:
        yield "%.7s: " % state + " ".join("%.7s" % ("%lf" % v[state]["prob"]) for v in V)

# Call the Viterbi function with defined parameters
viterbi(obs, states, start_p, trans_p, emit_p)
```
![image](https://github.com/Tudor-Gavriliuc/articles/assets/120340209/2905115f-f9f9-472f-82df-7887d43b3706)

## *A simple approach of speech recognition*

```
# Import required libraries
from gtts import gTTS  # For text-to-speech synthesis
import os
from os import path
from deep_translator import GoogleTranslator  # For text translation
import speech_recognition as sp  # For speech recognition

# Define a class named 'pevervodcik'
class pevervodcik:
    # Constructor with 'language' as a parameter
    def __init__(self, language):
        self.language = language

    # Method to capture audio input using a microphone
    def get_audio(self):
        mic = sp.Microphone(device_index=0)  # Microphone initialization
        r = sp.Recognizer()  # Speech recognizer initialization
        with mic as source:
            print("Spuneti acum")  # Prompt for user to speak
            audio = r.listen(source)  # Listen for audio input
        try:
            c = r.recognize_google(audio, language="ro-RO")  # Recognize spoken text using Google's API
        except:
            print("Something go wrong")  # Error message if recognition fails
        return c  # Return recognized text

    # Method to translate the provided text
    def translate(self, c):
        # Translate the text using GoogleTranslator with the target language specified in the constructor
        translated = GoogleTranslator(source='auto', target=self.language).translate(c)
        print(translated)  # Print translated text
        return translated  # Return translated text

    # Method to convert translated text to voice and play it
    def voice(self, trans):
        # Convert translated text to speech using gTTS (Google Text-to-Speech)
        myobj = gTTS(text=trans, lang=self.language, slow=False)
        myobj.save("welcome.mp3")  # Save the generated speech as an audio file
        os.system("welcome.mp3")  # Play the saved audio file

# Create an instance of 'pevervodcik' with target language 'en' (English)
Ion = pevervodcik('en')
# Capture audio input from the user
c = Ion.get_audio()
# Translate the captured text
trans = Ion.translate(c)
# Convert the translated text to speech and play it
Ion.voice(trans)

# Note: Instead of using a microphone, the code can also be modified to read from an existing audio file.
```
In the code above we have created a simple translator based on speech recognition algorithm. In our class ,,perevodcik,, we have 3 methods.

First method is responsible for recognizing the speech and saving it as an audio file.

Second method is responsible for translating the audio file according to the required language and saving it in an audio new file using GoogleTranslator library.

The last method plays the audio file using gTTS library.

## Conclusion

In conclusion, speech recognition stands as a remarkable testament to the ever-evolving potential of human-technology symbiosis. Its transformative impact transcends mere convenience, delving deep into the realms of accessibility, communication, and innovation. What was once a futuristic concept has woven itself into the fabric of our daily lives, enriching the way we interact with devices, unlocking new horizons of productivity, and fostering inclusion.
